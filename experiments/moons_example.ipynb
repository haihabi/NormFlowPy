{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import normflowpy as nfp\n",
    "import datasets\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.distributions import MultivariateNormal\n",
    "from experiments.functions import run_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Device is set to:cuda\n"
     ]
    }
   ],
   "source": [
    "dataset_type = datasets.DatasetType.MOONS\n",
    "n_training_samples = 50000\n",
    "n_validation_samples = 10000\n",
    "n_flow_blocks = 3\n",
    "batch_size = 32\n",
    "n_epochs = 50\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Current Working Device is set to:\" + str(device))\n",
    "training_data = datasets.get_dataset(dataset_type, n_training_samples)\n",
    "training_dataset_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_size,\n",
    "                                                      shuffle=True, num_workers=0)\n",
    "\n",
    "validation_data = datasets.get_dataset(dataset_type, n_validation_samples)\n",
    "validation_dataset_loader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size,\n",
    "                                                        shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Glow Normalizing Flow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d28ae4b11dd64d368451c252971ad42a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Epoch with training loss:0.8529789548841563 and validtion loss:0.6852377616940215\n",
      "End Epoch with training loss:0.625314123151551 and validtion loss:0.5498482723015184\n",
      "End Epoch with training loss:0.5749874504727579 and validtion loss:0.519436664474658\n",
      "End Epoch with training loss:0.5042935252494715 and validtion loss:0.47712750271105536\n",
      "End Epoch with training loss:0.46690563599192325 and validtion loss:0.4494500444910397\n",
      "End Epoch with training loss:0.4519591732659709 and validtion loss:0.4344318841402523\n",
      "End Epoch with training loss:0.4406210657445117 and validtion loss:0.428698668464685\n",
      "End Epoch with training loss:0.43800466336543487 and validtion loss:0.440927479832698\n",
      "End Epoch with training loss:0.42571755149237867 and validtion loss:0.4086377393132962\n",
      "End Epoch with training loss:0.41318915678534995 and validtion loss:0.3969904655656114\n",
      "End Epoch with training loss:0.4118412152571474 and validtion loss:0.38973174836879343\n",
      "End Epoch with training loss:0.4163065085083875 and validtion loss:0.39231539262940707\n",
      "End Epoch with training loss:0.39320436018976124 and validtion loss:0.3804083369410457\n",
      "End Epoch with training loss:0.3958701555660651 and validtion loss:0.3764353591127518\n",
      "End Epoch with training loss:0.3889959336795337 and validtion loss:0.3753593610212826\n",
      "End Epoch with training loss:0.3876360187680006 and validtion loss:0.37039579589146016\n",
      "End Epoch with training loss:0.3810516723591932 and validtion loss:0.3650995408669828\n",
      "End Epoch with training loss:0.38084524319824775 and validtion loss:0.37435486550910024\n",
      "End Epoch with training loss:0.3875815537463223 and validtion loss:0.3618916952000639\n",
      "End Epoch with training loss:0.37245199266375445 and validtion loss:0.3566368981576956\n",
      "End Epoch with training loss:0.3697135296488754 and validtion loss:0.35620561546791857\n",
      "End Epoch with training loss:0.369530514130513 and validtion loss:0.3585341178094998\n",
      "End Epoch with training loss:0.37098807710927795 and validtion loss:0.3510468300824729\n",
      "End Epoch with training loss:0.36485362120606696 and validtion loss:0.3530580795611056\n",
      "End Epoch with training loss:0.3654492939616806 and validtion loss:0.3559210633698363\n",
      "End Epoch with training loss:0.36820257127628214 and validtion loss:0.361436774412664\n",
      "End Epoch with training loss:0.35920061438951595 and validtion loss:0.3488426240393148\n",
      "End Epoch with training loss:0.3793594748430045 and validtion loss:0.360303451458867\n",
      "End Epoch with training loss:0.35834798704944815 and validtion loss:0.346545903684613\n",
      "End Epoch with training loss:0.35150939176575313 and validtion loss:0.3456488682058292\n",
      "End Epoch with training loss:0.36586884073603254 and validtion loss:0.3614398472891829\n",
      "End Epoch with training loss:0.3512922252360934 and validtion loss:0.34383901625205154\n",
      "End Epoch with training loss:0.35849963854557415 and validtion loss:0.3468008771681557\n",
      "End Epoch with training loss:0.34796532639615135 and validtion loss:0.3398673011186405\n",
      "End Epoch with training loss:0.3517105635338995 and validtion loss:0.34063635233301703\n",
      "End Epoch with training loss:0.34223720449441836 and validtion loss:0.3392872978418399\n",
      "End Epoch with training loss:0.3442440792603593 and validtion loss:0.32884323130400417\n",
      "End Epoch with training loss:0.38469509062519913 and validtion loss:0.37010460696852626\n",
      "End Epoch with training loss:0.35510322894908186 and validtion loss:0.347776002491625\n",
      "End Epoch with training loss:0.3447137486072816 and validtion loss:0.3377919367041451\n",
      "End Epoch with training loss:0.3412302066920586 and validtion loss:0.3376406386447029\n",
      "End Epoch with training loss:0.34843684239068706 and validtion loss:0.3348930743270027\n",
      "End Epoch with training loss:0.3383681081325979 and validtion loss:0.32782931368762314\n",
      "End Epoch with training loss:0.347869522464405 and validtion loss:0.33805021910240857\n",
      "End Epoch with training loss:0.3402822310830719 and validtion loss:0.3321067090042102\n",
      "End Epoch with training loss:0.3500736220784471 and validtion loss:0.3312286945958488\n",
      "End Epoch with training loss:0.3366652278137832 and validtion loss:0.32513625110490635\n",
      "End Epoch with training loss:0.34445745582315146 and validtion loss:0.3281395581012336\n",
      "End Epoch with training loss:0.34333079313038256 and validtion loss:0.3316729198724698\n",
      "End Epoch with training loss:0.34047381840145746 and validtion loss:0.35727252251804825\n"
     ]
    }
   ],
   "source": [
    "dim = training_data.dim()  # get data dim\n",
    "base_distribution = MultivariateNormal(torch.zeros(dim, device=device),\n",
    "                                       torch.eye(dim, device=device))  # generate a class for base distribution\n",
    "flows = []\n",
    "for i in range(n_flow_blocks):\n",
    "    flows.append(\n",
    "        nfp.flows.ActNorm(dim=dim))\n",
    "    flows.append(\n",
    "        nfp.flows.InvertibleFullyConnected(dim=dim))\n",
    "    flows.append(\n",
    "        nfp.flows.AffineCoupling(x_shape=[dim], parity=i % 2, net_class=nfp.base_nets.generate_mlp_class(), nh=32))\n",
    "flow = nfp.NormalizingFlowModel(base_distribution, flows).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set Optimizer and run training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(flow.parameters(), lr=1e-4)\n",
    "run_training(n_epochs, training_dataset_loader, validation_dataset_loader, flow, optimizer, device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plot probability Map"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_probability_map(n_points, in_x_min, in_x_max, in_y_min, in_y_max, in_flow_model, in_device):\n",
    "    results = []\n",
    "    for x_tag in torch.linspace(in_x_min, in_x_max, n_points):\n",
    "        _results_y = []\n",
    "        for y_tag in torch.linspace(in_y_min, in_y_max, n_points):\n",
    "            d = torch.stack([x_tag, y_tag]).reshape([1, -1]).to(in_device)\n",
    "            _results_y.append(in_flow_model.nll(d).item())\n",
    "        results.append(_results_y)\n",
    "    return np.exp(-np.asarray(results)).T\n",
    "\n",
    "\n",
    "x_min = -1.4\n",
    "x_max = 2.1\n",
    "y_min = -1\n",
    "y_max = 1.5\n",
    "res = generate_probability_map(200, x_min, x_max, y_min, y_max, flow, device)\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(2)\n",
    "x = np.linspace(x_min, x_max, 200)\n",
    "y = np.linspace(y_min, y_max, 200)\n",
    "\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "im = ax0.pcolormesh(xx, yy, res)\n",
    "\n",
    "ax1.plot(training_data[:, 0], training_data[:, 1], \"o\")\n",
    "ax1.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}